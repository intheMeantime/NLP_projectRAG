{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/cookbooks/upstage/Solar-Full-Stack LLM-101/02_prompt_engineering.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "## Overview  \n",
    "In this exercise, we will explore prompt engineering within the Solar framework. Prompt engineering is a crucial technique in leveraging large language models effectively by crafting prompts that elicit the desired responses from the model. This tutorial will walk you through various strategies for creating and refining prompts to optimize the performance of Solar in different NLP tasks.\n",
    " \n",
    "## Purpose of the Exercise\n",
    "The purpose of this exercise is to equip users with practical skills in prompt engineering. By the end of this tutorial, users will be able to design effective prompts, understand the impact of prompt variations, and enhance the accuracy and relevance of responses generated by the Solar LLM. This foundational knowledge is essential for advanced applications and fine-tuning of language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title set API key\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython import get_ipython\n",
    " \n",
    "upstage_api_key_env_name = \"up_svEvm8FOA6g9OILcC8oqs9bg4RSB7\"\n",
    "\n",
    "\n",
    "def load_env():\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        # Running in Google Colab\n",
    "        from google.colab import userdata\n",
    "\n",
    "        upstage_api_key = userdata.get(upstage_api_key_env_name)\n",
    "        return os.environ.setdefault(upstage_api_key_env_name, upstage_api_key)\n",
    "    else:\n",
    "        # Running in local Jupyter Notebook\n",
    "        from dotenv import load_dotenv\n",
    "\n",
    "        load_dotenv()\n",
    "        return os.environ.get(upstage_api_key_env_name)\n",
    "\n",
    "\n",
    "UPSTAGE_API_KEY = load_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://api.python.langchain.com/en/latest/chat_models/langchain_upstage.chat_models.ChatUpstage.html\n",
    "\n",
    "미니 모델이 맞대요 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of France is Paris.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 16, 'total_tokens': 23, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'solar-mini-240612', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-8c66c301-9df0-498f-81f9-3c0fa128b5ac-0' usage_metadata={'input_tokens': 16, 'output_tokens': 7, 'total_tokens': 23, 'input_token_details': {}, 'output_token_details': {}}\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Quick hello world \n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "llm = ChatUpstage(api_key=upstage_api_key_env_name)\n",
    "response = llm.invoke(\"What is the capital of France\")\n",
    "\n",
    "print(response) # 답변 외에도 token 수,, 거절 했는지 여부 등 여러 정보가 출력됨\n",
    "print(response.content) # 얘가 generate 한 content만 뽑아내는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!Answer! The capital of South Korea is Seoul. The capital of North Korea is Pyongyang. Remember, these are two different countries with different capitals!\n"
     ]
    }
   ],
   "source": [
    "# Chat prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"), # defult prompt # 여기다가 형식 요구해도 되고\n",
    "        (\"human\", \"What is the capital of France?\"),\n",
    "        (\"ai\", \"!Answer! It's Paris!!\"), # chat history를 넣어주는 것,, in-context learning # 여기다가 form 넣어줘도 되고\n",
    "        (\"human\", \"What about Korea?\"), #여따 해도 되고 ~ 여러가지 잇음~\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. define chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = chat_prompt | llm \n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cafeteria started with 23 apples. They used 20 of them, so they had 23 - 20 = 3 apples left. Then they bought 6 more apples, so they had 3 + 6 = 9 apples. \n",
      "\n",
      "The final answer is: \\boxed{9}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template( # basic prompt\n",
    "    \"\"\"\n",
    "Q: The cafeteria had 23 apples. \n",
    "If they used 20 to make lunch and bought 6 more, \n",
    "how many apples do they have?\n",
    "\n",
    "A: the answer is \n",
    "\"\"\"\n",
    "    # zero-shot learning\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.\n",
      "\n",
      "The cafeteria started with 23 apples. They used 20 apples for lunch, so they had 23 - 20 = 3 apples left. Then they bought 6 more apples, so they had 3 + 6 = 9 apples. However, since they used 20 apples for lunch, they would have 9 - 20 = -11 apples, which is not possible. Therefore, the cafeteria would have run out of apples before they could make lunch for everyone.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "\n",
    "A: The answer is 11.\n",
    "\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\n",
    "A: the answer is\n",
    "\"\"\"\n",
    ") # one-shot prompting\n",
    "chain = prompt_template | llm\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CoT](figures/cot.webp)\n",
    "\n",
    "from https://arxiv.org/abs/2201.11903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cafeteria started with 23 apples. They used 20, so they had 3 apples left. They bought 6 more, so they had 9 apples. The answer is 9.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
    "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"\"\"\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Zero-Shot COT](figures/zero-cot.webp)\n",
    "\n",
    "From https://arxiv.org/abs/2205.11916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: First, we need to determine the number of apples used for lunch. The cafeteria used 20 apples.\n",
      "Step 2: Next, we calculate the number of apples remaining after using 20. The cafeteria started with 23 apples, so after using 20, they would have 23 - 20 = 3 apples left.\n",
      "Step 3: Finally, we add the 6 apples they bought to the 3 apples remaining. This gives us a total of 3 + 6 = 9 apples.\n",
      "\n",
      "Answer: The cafeteria has 9 apples.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "\n",
    "A: The answer is 11.\n",
    "\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "\n",
    "A: Let's think step by step.\n",
    "\"\"\"\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## divide and conquer\n",
    "![divideandconquer](figures/divideandconquer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is SOLAR 10.7B and what are its key features?\n",
      "2. How does depth up-scaling (DUS) differ from other LLM up-scaling methods, and what are its advantages?\n",
      "3. How does SOLAR 10.7B-Instruct compare to Mixtral-8x7B-Instruct in terms of instruction-following capabilities?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide three questions from the following text:\n",
    "    ---\n",
    "    We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "    demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "    Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "    we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "    which encompasses depthwise scaling and continued pretraining.\n",
    "    In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "    DUS does not require complex changes to train and inference efficiently. \n",
    "    We show experimentally that DUS is simple yet effective \n",
    "    in scaling up high-performance LLMs from small ones. \n",
    "    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "    a variant fine-tuned for instruction-following capabilities, \n",
    "    surpassing Mixtral-8x7B-Instruct. \n",
    "    SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "    promoting broad access and application in the LLM field.\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SOLAR 10.7B\n",
      "2. depth up-scaling (DUS)\n",
      "3. instruction-following capabilities\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please extract three keywords from the following text:\n",
    "    ---\n",
    "    We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "    demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "    Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "    we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "    which encompasses depthwise scaling and continued pretraining.\n",
    "    In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "    DUS does not require complex changes to train and inference efficiently. \n",
    "    We show experimentally that DUS is simple yet effective \n",
    "    in scaling up high-performance LLMs from small ones. \n",
    "    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "    a variant fine-tuned for instruction-following capabilities, \n",
    "    surpassing Mixtral-8x7B-Instruct. \n",
    "    SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "    promoting broad access and application in the LLM field.\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the method for scaling large language models (LLMs) called, which encompasses depthwise scaling and continued pretraining, and does not require complex changes to train and inference efficiently?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide one question from the following text \n",
    "    regarding \"Depth up-scaling (DUS)\":\n",
    "    \n",
    "    ---\n",
    "    We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "    demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "    Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "    we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "    which encompasses depthwise scaling and continued pretraining.\n",
    "    In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "    DUS does not require complex changes to train and inference efficiently. \n",
    "    We show experimentally that DUS is simple yet effective \n",
    "    in scaling up high-performance LLMs from small ones. \n",
    "    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "    a variant fine-tuned for instruction-following capabilities, \n",
    "    surpassing Mixtral-8x7B-Instruct. \n",
    "    SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "    promoting broad access and application in the LLM field.\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "response = chain.invoke({})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the chicken join a band?\\n\\nBecause it had the drumsticks!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | llm | StrOutputParser()\n",
    "chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the beef go to the gym? To beef up!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"adjective\": \"funny\", \"content\": \"beef\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the name of the large language model with 10.7 billion parameters introduced in the text?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide one question from the following text \n",
    "    regarding \"{keyword}\":\n",
    "    \n",
    "    ---\n",
    "    {text}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "keyword = \"DUS\"\n",
    "text = \"\"\"\n",
    "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters, \n",
    "    demonstrating superior performance in various natural language processing (NLP) tasks. \n",
    "    Inspired by recent efforts to efficiently up-scale LLMs, \n",
    "    we present a method for scaling LLMs called depth up-scaling (DUS), \n",
    "    which encompasses depthwise scaling and continued pretraining.\n",
    "    In contrast to other LLM up-scaling methods that use mixture-of-experts, \n",
    "    DUS does not require complex changes to train and inference efficiently. \n",
    "    We show experimentally that DUS is simple yet effective \n",
    "    in scaling up high-performance LLMs from small ones. \n",
    "    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct, \n",
    "    a variant fine-tuned for instruction-following capabilities, \n",
    "    surpassing Mixtral-8x7B-Instruct. \n",
    "    SOLAR 10.7B is publicly available under the Apache 2.0 license, \n",
    "    promoting broad access and application in the LLM field.\n",
    "\"\"\"\n",
    "chain.invoke({\"keyword\": keyword, \"text\": text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* https://platform.openai.com/docs/guides/prompt-engineering \n",
    "* https://docs.anthropic.com/claude/docs/intro-to-prompting \n",
    "* https://smith.langchain.com/hub "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Law Textbook DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "file = open(\"info/api.txt\", \"r\")\n",
    "api_key = file.read()\n",
    "file.close()\n",
    "file = open(\"info/datapath.txt\", \"r\")\n",
    "data_path = file.read()\n",
    "file.close()\n",
    "file = open(\"info/resultspath.txt\", \"r\")\n",
    "results_path = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "import os\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "UPSTAGE_API_KEY = api_key\n",
    "\n",
    "# Only split \"civil_procedure_casebook\" into chunks of 4000 tokens\n",
    "booklist = [\"jurisprudence\", \"international_law\", \"criminal_law_casebook\", \"torts_casebook\", \"civil_procedure_casebook\"]\n",
    "book = booklist[4]\n",
    "\n",
    "layzer = UpstageLayoutAnalysisLoader(api_key=UPSTAGE_API_KEY,file_path=os.path.join(data_path, f'law_textbook/{book}.pdf'), output_type=\"text\")\n",
    "docs = layzer.load()  # or layzer.lazy_load()\n",
    "\n",
    "\n",
    "# Query-specific embedding model\n",
    "query_embeddings = UpstageEmbeddings(api_key=api_key, model=\"solar-embedding-1-large-query\")\n",
    "# Sentence-specific embedding model\n",
    "passage_embeddings = UpstageEmbeddings(api_key=api_key, model=\"solar-embedding-1-large-passage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "chunk_size = 5000\n",
    "chunk_overlap = chunk_size * 0.1\n",
    "\n",
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap, language=Language.HTML\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(\"Splits:\", len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(splits[5])\n",
    "len(splits[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = []\n",
    "for index in range(5,len(splits)) : # Get the context\n",
    "    chunk.append(splits[index].page_content)\n",
    "\n",
    "    # Exclude the initial splits as they are irrelevant to the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_documents = passage_embeddings.embed_documents(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(data_path+f'embedding/chunked{chunk_size}_Textbook_{book}', np.array(chunk))\n",
    "np.save(data_path+f'embedding/embedded{chunk_size}_Textbook_{book}', np.array(embedded_documents))\n",
    "\n",
    "print(\"1D list saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to load the saved embedding list:\n",
    "```\n",
    "# Load the .npy file (type == numpy)\n",
    "loaded_array = np.load('my_list.npy')\n",
    "\n",
    "# Convert the NumPy array to a Python list\n",
    "restored_list = loaded_array.tolist()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "booklist = [\"jurisprudence\", \"international_law\", \"criminal_law_casebook\", \"torts_casebook\", \"civil_procedure_casebook\"]\n",
    "\n",
    "full_law = []\n",
    "full_law_embed = []\n",
    "for book in booklist[:-1] :\n",
    "    # Load the .npy file (type == numpy)\n",
    "    textbookDB = np.load(data_path+f'embedding/chunked5000_Textbook_{book}.npy')\n",
    "    textbookDB = textbookDB.tolist()\n",
    "    print(len(textbookDB))\n",
    "\n",
    "    textbookDB_embed = np.load(data_path+f'embedding/embedded5000_Textbook_{book}.npy')\n",
    "    textbookDB_embed = textbookDB_embed.tolist()\n",
    "\n",
    "    for idx in range(len(textbookDB)) :\n",
    "        full_law.append(textbookDB[idx])\n",
    "        full_law_embed.append(textbookDB_embed[idx])\n",
    "\n",
    "textbookDB = np.load(data_path+f'embedding/chunked4000_Textbook_civil_procedure_casebook.npy')\n",
    "textbookDB = textbookDB.tolist()\n",
    "print(len(textbookDB))\n",
    "\n",
    "textbookDB_embed = np.load(data_path+f'embedding/embedded4000_Textbook_civil_procedure_casebook.npy')\n",
    "textbookDB_embed = textbookDB_embed.tolist()\n",
    "\n",
    "for idx in range(len(textbookDB)) :\n",
    "    full_law.append(textbookDB[idx])\n",
    "    full_law_embed.append(textbookDB_embed[idx])\n",
    "\n",
    "\n",
    "np.save(data_path+f'embedding/full_law_textbook', np.array(chunk))\n",
    "np.save(data_path+f'embedding/full_law_textbook_embed', np.array(embedded_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_law)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoonnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

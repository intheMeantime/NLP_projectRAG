{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -1. Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "UPSTAGE_API_KEY = \"up_e0SGJQIH0pC9VHFVwb6TXV7TRUKoh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build the Business DB (perform chunking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Load the \"Introduction to Marketing.pdf\" and perform chunking (chunks0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layzer = UpstageLayoutAnalysisLoader(api_key=UPSTAGE_API_KEY,file_path=\"/Users/hongjiyoung/NLP/Term_Project/final/db_files/Introduction To Marketing.pdf\", output_type=\"text\") \n",
    "\n",
    "docs0 = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximately 140,000 tokens\n",
    "\n",
    "# Set the chunk_size and chunk_overlap values for chunking.\n",
    "chunk_size = 5000 \n",
    "chunk_overlap = 500 \n",
    "\n",
    "# Set the Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    language=Language.HTML\n",
    ")\n",
    "chunks0 = [doc.page_content for doc in text_splitter.split_documents(docs0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Load the \"Principles of Management-OP.pdf\" and perform chunking (chunks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layzer = UpstageLayoutAnalysisLoader(api_key=UPSTAGE_API_KEY,file_path=\"/Users/hongjiyoung/NLP/Term_Project/baseline/mmlu/business/PrinciplesofManagement-OP 2.pdf\", output_type=\"text\") \n",
    "\n",
    "docs1 = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximately 1.8 million tokens\n",
    "\n",
    "# Set the chunk_size and chunk_overlap values for chunking.\n",
    "chunk_size = 9000 \n",
    "chunk_overlap = 900     \n",
    "\n",
    "# Set the Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    language=Language.HTML\n",
    ")\n",
    "chunks1 = [doc.page_content for doc in text_splitter.split_documents(docs1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Load the \"Corporate Finance.pdf\" and perform chunking (chunks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layzer = UpstageLayoutAnalysisLoader(api_key=UPSTAGE_API_KEY,file_path=\"/Users/hongjiyoung/NLP/Term_Project/final/db_files/Corporate Finance 2.pdf\", output_type=\"text\") \n",
    "\n",
    "docs2 = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximately 3.4 million tokens\n",
    "\n",
    "# Set the chunk_size and chunk_overlap values for chunking.\n",
    "chunk_size = 5000 \n",
    "chunk_overlap = 500   \n",
    "\n",
    "# Set the Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    language=Language.HTML\n",
    ")\n",
    "chunks2 = [doc.page_content for doc in text_splitter.split_documents(docs2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Load the \"Business Ethics - Concepts and Cases.pdf\" and perform chunking (chunks3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layzer = UpstageLayoutAnalysisLoader(api_key=UPSTAGE_API_KEY,file_path=\"/Users/hongjiyoung/NLP/Term_Project/final/db_files/Ethics- Concepts and Cases.pdf\", output_type=\"text\") \n",
    "\n",
    "docs3 = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximately 1.8 million tokens\n",
    "\n",
    "# Set the chunk_size and chunk_overlap values for chunking.\n",
    "chunk_size = 7000 \n",
    "chunk_overlap = 700     \n",
    "\n",
    "# Set the Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    language=Language.HTML\n",
    ")\n",
    "chunks3 = [doc.page_content for doc in text_splitter.split_documents(docs3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Load the \"Marketing Management.pdf\" and perform chunking (chunks4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layzer = UpstageLayoutAnalysisLoader(api_key=UPSTAGE_API_KEY,file_path=\"/Users/hongjiyoung/NLP/Term_Project/final/db_files/Marketing Management 15th Edition by Philip Kotler ( PDFDrive )-2.pdf\", output_type=\"text\") \n",
    "\n",
    "docs4 = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximately 3.6 million tokens\n",
    "\n",
    "# Set the chunk_size and chunk_overlap values for chunking.\n",
    "chunk_size = 7000 \n",
    "chunk_overlap = 700     \n",
    "\n",
    "# Set the Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    language=Language.HTML\n",
    ")\n",
    "chunks4 = [doc.page_content for doc in text_splitter.split_documents(docs4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a DataFrame for the chunks and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the chunks into a DataFrame ex.) chunks0, chunks1, chunks2,...\n",
    "# chunks0 : Introduction To Marketing.pdf\n",
    "# chunks1 : Principles of Management-OP.pdf\n",
    "# chunks2 : Corporate Finance.pdf\n",
    "# chunks3 : Business Ethics - Concepts and Cases.pdf\n",
    "# chunks4 : Marketing Management.pdf\n",
    "\n",
    "data = {'chunks' : chunks0}  \n",
    "chunks_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an ‘Index’ column with index numbers starting from 0.\n",
    "chunks_df.insert(0, 'Index', range(len(chunks_df)))  # Add the ‘Index’ column at the first column position.\n",
    "chunks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks_df['chunks'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a CSV file\n",
    "chunks_df.to_csv('db_business_MK_cs5000_co500.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perform DB embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv(\"/Users/hongjiyoung/NLP/Term_Project/final/db_files/db_business_MK_cs5000_co500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_save_embeddings(df, embedding_model, output_file_name):\n",
    "    context = df['chunks'].tolist()\n",
    "    index = df['Index'].tolist()\n",
    "    \n",
    "    # List for storing the results\n",
    "    valid_context_embeddings = []\n",
    "    valid_indices = []\n",
    "\n",
    "    for i, text in enumerate(context):\n",
    "        try:\n",
    "            # Calculate the embeddings\n",
    "            embedding = embedding_model.embed_documents([text])  # Calculate embeddings one by one\n",
    "            valid_context_embeddings.append(embedding[0])  # Store the results\n",
    "            valid_indices.append(index[i])  # Store the indices\n",
    "        except Exception as e:\n",
    "            # Print the error message and skip\n",
    "            print(f\"Error with context at index {index[i]}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save the data using Pickle\n",
    "    with open(output_file_name, \"wb\") as f:\n",
    "        pickle.dump((valid_context_embeddings, valid_indices), f)\n",
    "    print(f\"Embeddings saved to '{output_file_name}'. {len(valid_context_embeddings)} items successfully processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = UpstageEmbeddings(api_key=UPSTAGE_API_KEY, model=\"solar-embedding-1-large\")\n",
    "output_file = \"bs_embeddings_MK_cs5000_co500.pkl\"\n",
    "\n",
    "calculate_and_save_embeddings(db, embedding_model, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge the DB & Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file.\n",
    "df0 = pd.read_csv('/Users/hongjiyoung/NLP/Term_Project/final/db_files/db_business_MK_cs5000_co500.csv')  # The first DataFrame\n",
    "df1 = pd.read_csv('//Users/hongjiyoung/NLP/Term_Project/final/db_files/db_business_PM_cs9000_co900.csv')  # The second DataFrame\n",
    "df2 = pd.read_csv('/Users/hongjiyoung/NLP/Term_Project/final/db_files/db_business_CF_cs5000_co500.csv')  # The third DataFrame\n",
    "df3 = pd.read_csv('/Users/hongjiyoung/NLP/Term_Project/final/db_files/db_business_BE_cs7000_co700.csv')  # The fourth DataFrame\n",
    "df4 = pd.read_csv('/Users/hongjiyoung/NLP/Term_Project/final/db_files/db_business_MM_cs7000_co700.csv')  # The fifth DataFrame\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = pd.concat([df0, df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Reindex the “Index” column starting from 0\n",
    "merged_df[\"Index\"] = range(len(merged_df))\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a new CSV file\n",
    "merged_df.to_csv('db_business_merged6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the five pkl files\n",
    "file0 = \"/Users/hongjiyoung/NLP/Term_Project/final/db_files/bs_embeddings_MK_cs5000_co500.pkl\"\n",
    "file1 = \"/Users/hongjiyoung/NLP/Term_Project/final/db_files/bs_embeddings_PM_cs9000_co900.pkl\"\n",
    "file2 = \"/Users/hongjiyoung/NLP/Term_Project/final/db_files/bs_embeddings_BE_cs7000_co700.pkl\"\n",
    "file3 = \"/Users/hongjiyoung/NLP/Term_Project/final/db_files/bs_embeddings_BE_cs7000_co700.pkl\"\n",
    "file4 = \"/Users/hongjiyoung/NLP/Term_Project/final/db_files/bs_embeddings_BE_cs7000_co700.pkl\"\n",
    "\n",
    "output_file = \"bs_embeddings_merged6.pkl\"\n",
    "\n",
    "# Load the first file\n",
    "with open(file0, \"rb\") as f:\n",
    "    embeddings0, indices0 = pickle.load(f)\n",
    "\n",
    "# Load the second file\n",
    "with open(file1, \"rb\") as f:\n",
    "    embeddings1, indices1 = pickle.load(f)\n",
    "\n",
    "# Load the third file\n",
    "with open(file2, \"rb\") as f:\n",
    "    embeddings2, indices2 = pickle.load(f)\n",
    "\n",
    "# Load the fourth file\n",
    "with open(file3, \"rb\") as f:\n",
    "    embeddings3, indices3 = pickle.load(f)\n",
    "\n",
    "# Load the fifth file\n",
    "with open(file4, \"rb\") as f:\n",
    "    embeddings4, indices4 = pickle.load(f)\n",
    "\n",
    "# Merge the data\n",
    "merged_embeddings = embeddings0 + embeddings1 + embeddings2 + embeddings3 + embeddings4 # Merge the embedding lists\n",
    "merged_indices = indices0 + indices1 + indices2 + indices3  + indices4   # 병합된 데이터를 저장\n",
    "merged_indices = [i for i in range(len(merged_indices))]\n",
    "\n",
    "# Save the merged data\n",
    "with open(output_file, \"wb\") as f:\n",
    "    pickle.dump((merged_embeddings, merged_indices), f)\n",
    "\n",
    "print(f\"Two pickle files merged and saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

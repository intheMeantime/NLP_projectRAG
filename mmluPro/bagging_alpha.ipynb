{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhMCta6KU4Yv"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/cookbooks/upstage/Solar-Full-Stack LLM-101/05_3_OracleDB.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG6F1kAYU4Yx"
   },
   "source": [
    "# EWHA bagging _ alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vnLzEVF8CysE"
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "file = open(\"info/api.txt\", \"r\")\n",
    "api_key = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"info/datapath.txt\", \"r\")\n",
    "data_path = file.read()\n",
    "file.close()\n",
    "\n",
    "file = open(\"info/resultspath.txt\", \"r\")\n",
    "results_path = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "import time\n",
    "\n",
    "# 쿼리 전용 임베딩 모델\n",
    "query_embeddings = UpstageEmbeddings(api_key=api_key, model=\"solar-embedding-1-large-query\")\n",
    "\n",
    "# 문장 전용 임베딩 모델\n",
    "passage_embeddings = UpstageEmbeddings(api_key=api_key, model=\"solar-embedding-1-large-passage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3ZAq6pyGhXLq"
   },
   "outputs": [],
   "source": [
    "# funcion to extract an answer from response\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_answer(response):\n",
    "    \"\"\"\n",
    "    extracts the answer from the response using a regular expression.\n",
    "    expected format: \"[ANSWER]: (A) convolutional networks\"\n",
    "\n",
    "    if there are any answers formatted like the format, it returns None.\n",
    "    \"\"\"\n",
    "    pattern = r\"\\[ANSWER\\]:\\s*\\((A|B|C|D|E)\\)\"  # Regular expression to capture the answer letter and text\n",
    "    match = re.search(pattern, response)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1) # Extract the letter inside parentheses (e.g., A)\n",
    "    else:\n",
    "        return extract_again(response)\n",
    "\n",
    "def extract_again(response):\n",
    "    pattern = r\"\\b[A-J]\\b(?!.*\\b[A-J]\\b)\"\n",
    "    match = re.search(pattern, response)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. build DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmCwvoB1zH3H",
    "outputId": "edee142c-257a-48ce-8d3b-4e2fbfa4b081"
   },
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "UPSTAGE_API_KEY = api_key\n",
    "\n",
    "# .npy 파일 로드 (타입==넘파이)\n",
    "textbookDB = np.load(data_path+f'embedding/full_philosophy_textbook.npy')\n",
    "textbookDB = textbookDB.tolist()\n",
    "\n",
    "textbookDB_embed = np.load(data_path+f'embedding/full_philosophy_textbook_embed.npy')\n",
    "textbookDB_embed = textbookDB_embed.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. test set 갖고오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0qSi7C5xeBVt"
   },
   "outputs": [],
   "source": [
    "# read samples.csv file\n",
    "import pandas as pd\n",
    "\n",
    "def read_data(data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    prompts = data['prompts']\n",
    "    answers = data['answers']\n",
    "    # returns two lists: prompts and answers\n",
    "    return prompts, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b4kuh6HOeqMv"
   },
   "outputs": [],
   "source": [
    "prompts, answers = read_data(os.path.join(data_path, 'mmlupro_test_philosophy.csv'))\n",
    "testdata = pd.read_csv(data_path+'mmlupro_test_philosophy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nowtest = pd.DataFrame(columns=['index', 'embed_ques', 'question', 'prompts', 'answers', 'top1', 'top2', 'top3', 'top1_1pred','top1_2pred','top1_3pred', 'top2pred', 'top3pred', 'predict' ])\n",
    "\n",
    "for index, row in testdata.iterrows():\n",
    "    #if index == 100 : break # 일단 실험할 땐 100개 단위로 끊어서 가져옴\n",
    "    q = row.prompts\n",
    "    a = row.answers\n",
    "    question = q.partition('(A)')[0]\n",
    "    question = question.partition(') ')[2]\n",
    "    q = q.partition(') ')[2]\n",
    "    try : \n",
    "        embedded_query = query_embeddings.embed_query(question) # 질문만 받아와서 embedding 하기\n",
    "        nowtest.loc[len(nowtest)] = {'index':index, 'embed_ques' : embedded_query, 'question' : question, 'prompts' : q, 'answers' : a}\n",
    "\n",
    "    except :\n",
    "        print(f'pass: {index}')\n",
    "        continue \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nowtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "for idx, row in nowtest.iterrows() : # 질문 받아오기 \n",
    "\n",
    "    embed_ques= row.embed_ques\n",
    "\n",
    "    # 유사도 기준 내림차순 정렬\n",
    "    sorted_idx = (np.array(embed_ques) @ np.array(textbookDB_embed).T).argsort()[::-1]\n",
    "\n",
    "    nowtest.loc[idx, 'top1'] = textbookDB[sorted_idx[0]]\n",
    "    nowtest.loc[idx, 'top2'] = textbookDB[sorted_idx[1]]\n",
    "    nowtest.loc[idx, 'top3'] = textbookDB[sorted_idx[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try : del [[bagging_pred]]\n",
    "except : pass\n",
    "bagging_pred = pd.DataFrame(columns=['questionNum', 'answer', 'top1_1pred', 'top1_2pred', 'top1_3pred', 'top2pred', 'top3pred', 'predict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# first PREDICTION ##########\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    " \n",
    "llm = ChatUpstage(api_key = api_key)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    '''\n",
    "    \n",
    "    Please provide most correct answer. Let's think step by step.\n",
    "    \n",
    "    When translating the answer, DO NOT exlain anything. And you must also include the choice number like :\n",
    "    Answer : (Number) the answer choice\n",
    "    ---\n",
    "    \n",
    "    Question: {question}\n",
    "\n",
    "    Context: {context1}\n",
    "\n",
    "    Answer :\n",
    "    ---\n",
    "        \n",
    "    '''\n",
    "\n",
    ")\n",
    "ko_chain1 = prompt_template | llm\n",
    "\n",
    "for idx, row in nowtest.iterrows() :\n",
    "    #if idx == 100 : break\n",
    "    max_retries = 3  # 최대 재시도 횟수\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = ko_chain1.invoke({\"question\": row.prompts, \"context1\": row.top1}) # 선지 전까지 받아오기\n",
    "            nowtest.loc[idx, 'top1pred'] = response.content\n",
    "\n",
    "            generated_answer = extract_answer(response.content)\n",
    "            bagging_pred.loc[len(bagging_pred)] = {'questionNum': row.question, 'answer': row.answers, 'top1_1pred': generated_answer}\n",
    "            break\n",
    "\n",
    "        except Exception as e:  # API 호출 에러\n",
    "            retries += 1\n",
    "            print(f\"Error occurred: {e}. Retrying idx:{idx} - {retries}/{max_retries} after 10 seconds...\")\n",
    "            time.sleep(10)  # 10초 대기 후 재시도\n",
    "            if retries == max_retries:\n",
    "                print(f\"Failed after {max_retries} retries. Skipping this context.\")\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(2,4) : \n",
    "    for idx, row in nowtest.iterrows() :\n",
    "        #if idx == 100 : break\n",
    "        max_retries = 3  # 최대 재시도 횟수\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                response = ko_chain1.invoke({\"question\": row.prompts, \"context1\": row.top1}) # 선지 전까지 받아오기\n",
    "                nowtest.loc[idx, 'top1pred'] = response.content\n",
    "\n",
    "                generated_answer = extract_answer(response.content)\n",
    "                bagging_pred.loc[idx, f'top1_{i}pred'] = generated_answer\n",
    "                break\n",
    "\n",
    "            except Exception as e:  # API 호출 에러\n",
    "                retries += 1\n",
    "                print(f\"Error occurred: {e}. Retrying idx:{idx} - {retries}/{max_retries} after 10 seconds...\")\n",
    "                time.sleep(10)  # 10초 대기 후 재시도\n",
    "                if retries == max_retries:\n",
    "                    print(f\"Failed after {max_retries} retries. Skipping this context.\")\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# second PREDICTION ##########\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_upstage import ChatUpstage\n",
    "import time\n",
    " \n",
    "llm = ChatUpstage(api_key = api_key)\n",
    "    \n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    '''\n",
    "\n",
    "    Please provide most correct answer. Let's think step by step.\n",
    "    \n",
    "    When translating the answer, DO NOT exlain anything. And you must also include the choice number like :\n",
    "    Answer : (Number) the answer choice\n",
    "    ---\n",
    "    \n",
    "    Question: {question}\n",
    "\n",
    "    Context: {context1} {context2}\n",
    "\n",
    "    Answer :\n",
    "    ---\n",
    "    '''\n",
    "\n",
    ")\n",
    "ko_chain2 = prompt_template | llm\n",
    "\n",
    "for idx, row in nowtest.iterrows() :\n",
    "    #if idx == 100 : break\n",
    "    max_retries = 3  # 최대 재시도 횟수\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = ko_chain2.invoke({\"question\": row.prompts, \"context1\": row.top1, \"context2\":row.top2})\n",
    "            nowtest.loc[idx, 'top2pred'] = response.content\n",
    "\n",
    "            generated_answer = extract_answer(response.content)\n",
    "            bagging_pred.loc[idx, 'top2pred'] = generated_answer\n",
    "            break\n",
    "\n",
    "        except Exception as e:  # API 호출 에러\n",
    "            retries += 1\n",
    "            print(f\"Error occurred: {e}. Retrying idx:{idx} - {retries}/{max_retries} after 10 seconds...\")\n",
    "            time.sleep(10)  # 10초 대기 후 재시도\n",
    "            if retries == max_retries:\n",
    "                print(f\"Failed after {max_retries} retries. Skipping this context.\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# third PREDICTION ##########\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    " \n",
    "llm = ChatUpstage(api_key = api_key)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    '''\n",
    "    Please provide most correct answer. Let's think step by step.\n",
    "    \n",
    "    When translating the answer, DO NOT exlain anything. And you must also include the choice number like :\n",
    "    Answer : (Number) the answer choice\n",
    "    ---\n",
    "    \n",
    "    Question: {question}\n",
    "\n",
    "    Context: {context1} {context2} {context3}\n",
    "\n",
    "    Answer :\n",
    "    ---\n",
    "        \n",
    "    '''\n",
    "\n",
    ")\n",
    "ko_chain3 = prompt_template | llm\n",
    "\n",
    "for idx, row in nowtest.iterrows() :\n",
    "    #if idx == 100 : break\n",
    "    max_retries = 3  # 최대 재시도 횟수\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = ko_chain3.invoke({\"question\": row.prompts, \"context1\": row.top1, \"context2\":row.top2, \"context3\":row.top3}) # 선지 전까지 받아오기\n",
    "            nowtest.loc[idx, 'top3pred'] = response.content\n",
    "\n",
    "            generated_answer = extract_answer(response.content)\n",
    "            bagging_pred.loc[idx, 'top3pred'] = generated_answer\n",
    "            break\n",
    "\n",
    "        except Exception as e:  # API 호출 에러\n",
    "            retries += 1\n",
    "            print(f\"Error occurred: {e}. Retrying {retries}/{max_retries} after 10 seconds...\")\n",
    "            time.sleep(10)  # 10초 대기 후 재시도\n",
    "            if retries == max_retries:\n",
    "                print(f\"Failed after {max_retries} retries. Skipping this context.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "final_pred = []\n",
    "for idx, row in bagging_pred.iterrows() :\n",
    "\n",
    "    pred = []\n",
    "    pred.append(row.top1_1pred)\n",
    "    pred.append(row.top1_2pred)\n",
    "    pred.append(row.top1_3pred)\n",
    "    pred.append(row.top2pred)\n",
    "    pred.append(row.top3pred)\n",
    "\n",
    "    counts = Counter(pred)\n",
    "\n",
    "    prediction = counts.most_common(1)[0][0]\n",
    "    if prediction == None :\n",
    "        try : prediction = counts.most_common(2)[1][0]\n",
    "        except : pass\n",
    "    if prediction == None : prediction = 'A'\n",
    "    final_pred.append(prediction)\n",
    "    bagging_pred.loc[idx, 'predict'] = prediction\n",
    "\n",
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 정답 확인 + wrong 뽑아내기 ######\n",
    "\n",
    "# print accuracy\n",
    "\n",
    "cnt = 0\n",
    "wrong = []\n",
    "for idx, (answer, response) in enumerate(zip(answers, final_pred)):\n",
    "    print(\"-\"*10)\n",
    "    try : generated_answer = extract_answer(response)\n",
    "    except : pass\n",
    "    print(response)\n",
    "    # check\n",
    "    if generated_answer:\n",
    "        print(f\"idx: {idx} | generated answer: {generated_answer}, answer: {answer}\")\n",
    "    else:\n",
    "        print(\"extraction fail\")\n",
    "\n",
    "    if generated_answer == None:\n",
    "        wrong.append(idx+1)\n",
    "        bagging_pred.loc[idx, 'iswrong'] = 'X'\n",
    "        continue\n",
    "    \n",
    "    if generated_answer in answer:\n",
    "        cnt += 1\n",
    "    else : \n",
    "        wrong.append(idx+1)\n",
    "        bagging_pred.loc[idx, 'iswrong'] = 'X'\n",
    "        \n",
    "acc = cnt/len(answers)*100\n",
    "print(f\"acc: {acc}%\")\n",
    "print()\n",
    "print(\"wrong:\", wrong)\n",
    "bagging_pred.loc[len(bagging_pred), 'predict'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 탑 원으로만 확인  ######\n",
    "\n",
    "# print accuracy\n",
    "top1_pred = []\n",
    "for idx, row in bagging_pred.iterrows() :\n",
    "    top1 = row.top1_1pred\n",
    "    if top1 == None : top1 = 'A'\n",
    "    top1_pred.append(top1)\n",
    "\n",
    "print(top1_pred)\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "wrong = []\n",
    "for idx, (answer, response) in enumerate(zip(answers, top1_pred)):\n",
    "    print(\"-\"*10)\n",
    "    try : generated_answer = extract_answer(response)\n",
    "    except : pass\n",
    "    print(response)\n",
    "    # check\n",
    "    if generated_answer:\n",
    "        print(f\"idx: {idx} | generated answer: {generated_answer}, answer: {answer}\")\n",
    "    else:\n",
    "        print(\"extraction fail\")\n",
    "\n",
    "    if generated_answer == None:\n",
    "        wrong.append(idx+1)\n",
    "        bagging_pred.loc[idx, 'iswrong'] = 'X'\n",
    "        continue\n",
    "    \n",
    "    if generated_answer in answer:\n",
    "        cnt += 1\n",
    "    else : \n",
    "        wrong.append(idx+1)\n",
    "        bagging_pred.loc[idx, 'iswrong'] = 'X'\n",
    "        \n",
    "acc = cnt/len(answer)*100\n",
    "print(f\"acc: {acc}%\")\n",
    "print()\n",
    "print(\"wrong:\", wrong)\n",
    "bagging_pred.loc[len(bagging_pred), 'predict'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagging_pred.to_csv(results_path+'ewha_bagging01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try : del[[wdf]]\n",
    "except : pass\n",
    "wdf = bagging_pred[bagging_pred['iswrong']=='X']\n",
    "wdf.loc[len(bagging_pred), 'predict'] = acc\n",
    "\n",
    "wdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "yoonnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
